# ğŸ“Š Job Offer Aggregator with Candidate Profile Matching

ğŸ‡µğŸ‡± Polish version below

## Project Goal

This project aims to **automatically collect job offers** from the most popular job platforms in real time and filter them based on **user-defined preferences**.  

The system:
- searches for job offers,
- filters them by predefined criteria (e.g. **salary range, work mode, contract type**),
- calculates a technical **Skill Match**,
- generates a personalized summary **sent via email**.

The solution saves time and eliminates the need for manually browsing job portals.  
It showcases a **real-world application of data engineering** in the recruitment process.

---

## How It Works (Technical Overview)

### 1. Data Sources â€“ **Kafka Producers (Web Scraping)**

- Two scrapers use **Selenium** and **BeautifulSoup** to periodically gather the latest job offers from various platforms.
- Structured data are sending to the Kafka messaging system.
  
### 2. Data Streaming â€“ **Apache Kafka + Avro**

- Offers are sent to a **Kafka topic** (`raw_offers`).
- Messages are serialized using **Avro** and validated against a schema via **Schema Registry**.

![Screenshot 2025-07-1 at 11 33 25](https://github.com/user-attachments/assets/a694e5cb-62ae-40b4-911e-790b024aca95)

### 3. Stream Processing â€“ **Consumer (Python)**

A Python-based consumer reads and processes incoming messages:

- âœ… **Avro decoding** â€“ according to the defined schema  
- ğŸ”„ **Normalization** â€“ unifying company names, locations, contract types, and experience levels  
- ğŸ’± **Currency conversion** â€“ using **NBP API** to convert salaries to PLN (net/gross, hourly rates)  
- ğŸ§¹ **Initial filtering** â€“ e.g. removing duplicates or fully on-site offers  
- ğŸ’¾ **Data storage in MongoDB** â€“ each offer is saved as a document in a **NoSQL** database

![Screenshot 2025-07-1 at 11 37 23](https://github.com/user-attachments/assets/3c844cea-324a-46be-9bf2-7483399c8b95)

### 4. Analysis & Reporting â€“ **CRON, Pandas, HTML**

A CRON-based analytics module runs periodically (e.g. every 14 days) inside a Docker container:

- ğŸ“¥ Loads job offers from MongoDB (last 14 days)  
- ğŸ§© Filters based on user preferences (`requirements_config.yaml`)  
- ğŸ§  Calculates **Skill Match** â€“ comparing required vs. owned skills (must-have & nice-to-have)  
- ğŸ“Š Generates an **HTML report** with the top-matching offers  
- ğŸ“¬ Sends the report via email (**SMTP**)

<img width="338" alt="Screenshot 2025-07-1 at 11 48 13" src="https://github.com/user-attachments/assets/df9aec37-70ec-4c5c-a6d6-0de60afc3c56" />

---

## Technologies Used

- **Python** â€“ scraping, ETL, data processing (`pandas`, `requests`, `selenium`, `bs4`)
- **Apache Kafka + Avro + Schema Registry** â€“ data streaming & schema validation
- **MongoDB (NoSQL)** â€“ flexible document-based storage
- **Docker + Docker Compose** â€“ containerized architecture
- **CRON** â€“ scheduled task execution in Docker
- **NBP API** â€“ real-time currency conversion to PLN
- **HTML/CSS + SMTP** â€“ HTML report generation and email sending

---

## Summary

This project showcases a **complete real-time data lifecycle**:  
ğŸ“¥ **collection** â†’ ğŸ§ª **validation** â†’ ğŸ”„ **transformation** â†’ ğŸ“Š **analysis** â†’ ğŸ“§ **reporting**

The system is **modular, scalable, and easily extendable** â€” for example, to support new job platforms or advanced scoring models.

---

> ğŸ› ï¸ This project was built for educational and personal skill development purposes only.





# PL: 



# ğŸ“Š Agregator ofert pracy z dopasowaniem do wymagaÅ„ uÅ¼ytkownika

## Cel projektu

Celem projektu jest **automatyczne zbieranie ofert pracy** z najpopularniejszych platform rekrutacyjnych w czasie rzeczywistym oraz filtrowanie ich zgodnie z **preferencjami uÅ¼ytkownika**.  
System:

- przeszukuje ogÅ‚oszenia,
- dopasowuje je do zdefiniowanych kryteriÃ³w (np. **wynagrodzenie, tryb pracy, forma zatrudnienia**),
- oblicza dopasowanie umiejÄ™tnoÅ›ci technicznych (**Skill Match**),
- generuje spersonalizowane zestawienie i **wysyÅ‚a je e-mailem**.

RozwiÄ…zanie oszczÄ™dza czas i eliminuje koniecznoÅ›Ä‡ rÄ™cznego przeszukiwania portali z ogÅ‚oszeniami.  
Projekt prezentuje **praktyczne zastosowanie inÅ¼ynierii danych** w realnym scenariuszu rekrutacyjnym.

---

## Jak to dziaÅ‚a (warstwa techniczna)

### 1. Å¹rÃ³dÅ‚a danych â€“ **Producenci Kafka (Web Scraping)**

- Dwa scrapery wykorzystujÄ… **Selenium** i **BeautifulSoup**, by okresowo zbieraÄ‡ najnowsze oferty z rÃ³Å¼nych portali.
- Dane sÄ… strukturyzowane i wysyÅ‚ane do systemu kolejek (Kafka).

### 2. Strumieniowanie danych â€“ **Apache Kafka + Avro**

- Oferty trafiajÄ… do **Kafka Topic** (`raw_offers`).
- Komunikaty sÄ… serializowane przy uÅ¼yciu **Avro** i walidowane poprzez **Schema Registry**.
  
![Zrzut ekranu 2025-07-1 o 11 33 25](https://github.com/user-attachments/assets/a694e5cb-62ae-40b4-911e-790b024aca95)

### 3. Przetwarzanie w strumieniu â€“ **Consumer (Python)**

Consumer subskrybuje topic i przetwarza dane:

- âœ… **Dekodowanie Avro** â€“ zgodnie ze schematem danych  
- ğŸ”„ **Normalizacja** â€“ nazwy, lokalizacje, typy umÃ³w, poziomy doÅ›wiadczenia  
- ğŸ’± **Przeliczanie walut** â€“ na podstawie kursu z **API NBP**, stawki w PLN (netto/brutto)  
- ğŸ§¹ **Filtrowanie wstÄ™pne** â€“ np. usuwanie duplikatÃ³w, ofert tylko stacjonarnych  
- ğŸ’¾ **Zapis do MongoDB** â€“ dane trafiajÄ… jako dokumenty do bazy **NoSQL**
  
![Zrzut ekranu 2025-07-1 o 11 37 23](https://github.com/user-attachments/assets/3c844cea-324a-46be-9bf2-7483399c8b95)

### 4. Analiza i raportowanie â€“ **CRON, Pandas, HTML**

ModuÅ‚ dziaÅ‚a cyklicznie (co 14 dni) jako zadanie CRON w kontenerze Docker:

- ğŸ“¥ Pobiera dane z MongoDB (ostatnie 14 dni)  
- ğŸ§© Filtrowanie po wymaganiach uÅ¼ytkownika (z pliku `requirements_config.yaml`)  
- ğŸ§  Oblicza **Skill Match** â€“ analiza zgodnoÅ›ci must-have i nice-to-have  
- ğŸ“Š Tworzy **raport HTML** z listÄ… najlepiej dopasowanych ofert  
- ğŸ“¬ WysyÅ‚a raport e-mailem do uÅ¼ytkownika (**SMTP**)
  
<img width="338" alt="Zrzut ekranu 2025-07-1 o 11 48 13" src="https://github.com/user-attachments/assets/df9aec37-70ec-4c5c-a6d6-0de60afc3c56" />


---

## UÅ¼yte technologie

- **Python** â€“ scraping, ETL, analiza danych (`pandas`, `requests`, `selenium`, `bs4`)
- **Apache Kafka + Avro + Schema Registry** â€“ streaming i kontrola schematÃ³w
- **MongoDB (NoSQL)** â€“ elastyczne przechowywanie danych
- **Docker + Docker Compose** â€“ konteneryzacja aplikacji
- **CRON** â€“ harmonogram zadaÅ„ w kontenerze
- **NBP API** â€“ przeliczanie walut na PLN
- **HTML/CSS + SMTP** â€“ tworzenie i wysyÅ‚anie raportÃ³w

---

## Podsumowanie

Projekt realizuje **peÅ‚ny cykl Å¼ycia danych w czasie rzeczywistym**:  
ğŸ“¥ **pozyskiwanie** â†’ ğŸ§ª **walidacja** â†’ ğŸ”„ **transformacja** â†’ ğŸ“Š **analiza** â†’ ğŸ“§ **raportowanie**

RozwiÄ…zanie jest **modularne, skalowalne i gotowe do rozszerzeÅ„** â€” np. o kolejne ÅºrÃ³dÅ‚a ofert czy inne modele scoringowe.

---

> ğŸ› ï¸ Projekt zrealizowany w ramach rozwijania kompetencji i sÅ‚uÅ¼y wyÅ‚Ä…cznie celom edukacyjnym.

